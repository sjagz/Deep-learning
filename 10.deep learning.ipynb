{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [0], [0], [1]]\n",
      "cost:1.3007614612579346\n",
      "cost:0.6416905522346497\n",
      "cost:0.5189223289489746\n",
      "cost:0.4683036208152771\n",
      "cost:0.43108364939689636\n",
      "cost:0.40025073289871216\n",
      "cost:0.3740113377571106\n",
      "cost:0.3513678014278412\n",
      "cost:0.33160412311553955\n",
      "cost:0.3141796886920929\n",
      "정확도 =1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "#and 연산 \n",
    "\n",
    "x_anddata= [[0,0],\n",
    "            [0,1],\n",
    "            [1,0],\n",
    "            [1,1]]\n",
    "y_anddata=[[0],[0],[0],[1]]\n",
    "\n",
    "print(y_anddata)\n",
    "\n",
    "X = tf.placeholder(shape=[None,2],dtype = tf.float32  ) #행 숫자는 무상관\n",
    "Y = tf.placeholder(shape=[None,1],dtype = tf.float32  )\n",
    "\n",
    "W= tf.Variable(tf.random_normal([2,1]), name=\"weight\"  )\n",
    "b= tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "\n",
    "logit = tf.matmul(X,W)+b\n",
    "\n",
    "H=tf.sigmoid(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( logits=logit , labels=Y ))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_anddata,Y:y_anddata} )\n",
    "    if step%300 == 0 :\n",
    "        print(\"cost:{}\".format(cost_val))\n",
    "\n",
    "predict = tf.cast(H>0.5 ,dtype=tf.float32) #원래는 정수를 실수로 , 실수를 정수로 바꿔주는 cast 타입변환 1.0 0.0 으로 변환인듯?\n",
    "                                            #0.5보다 크면 1로 떨어짐\n",
    "correct = tf.equal(predict,Y) #실데이터와 예측한 것을 비교하는것 \n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct , dtype=tf.float32))\n",
    "\n",
    "print(\"정확도 ={}\".format(sess.run(accuracy ,feed_dict={X:x_anddata,Y:y_anddata} ) ))\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:4.508669376373291\n",
      "cost:0.004567247349768877\n",
      "cost:0.0023713859263807535\n",
      "cost:0.001627581543289125\n",
      "cost:0.0012495584087446332\n",
      "cost:0.0010194529313594103\n",
      "cost:0.0008640398737043142\n",
      "cost:0.0007517856429331005\n",
      "cost:0.0006666595581918955\n",
      "cost:0.0005997646367177367\n",
      "cost:0.0005457483348436654\n",
      "cost:0.0005011583562009037\n",
      "cost:0.00046375609235838056\n",
      "cost:0.000431897584348917\n",
      "cost:0.00040439952863380313\n",
      "cost:0.00038042888627387583\n",
      "cost:0.00035927712451666594\n",
      "cost:0.00034052622504532337\n",
      "cost:0.00032373613794334233\n",
      "cost:0.00030863448046147823\n",
      "정확도 =1.0\n"
     ]
    }
   ],
   "source": [
    "#multiple layer 를 이용해 kor 문제 해결 (2개이사의 레이어)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "x_data= [[0,0],\n",
    "        [0,1],\n",
    "        [1,0],\n",
    "        [1,1]]\n",
    "y_data=[[0],[0],[0],[1]]\n",
    "\n",
    "X = tf.placeholder(shape=[None,2],dtype = tf.float32  ) #행 숫자는 무상관\n",
    "Y = tf.placeholder(shape=[None,1],dtype = tf.float32  )\n",
    "\n",
    "\n",
    "W1= tf.Variable(tf.random_normal([2,256]), name=\"weight1\"  ) # \n",
    "b1= tf.Variable(tf.random_normal([256]),name=\"bias1\")\n",
    "\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "\n",
    "\n",
    "W2= tf.Variable(tf.random_normal([256,512]), name=\"weight1\"  ) # \n",
    "b2= tf.Variable(tf.random_normal([512]),name=\"bias1\")\n",
    "\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "\n",
    "W_3=tf.Variable(tf.random_normal([512,1]), name=\"weight3\"  )\n",
    "b_3=tf.Variable(tf.random_normal([1]),name=\"bias3\")\n",
    "\n",
    "#print(sess.run([W1],feed_dict={X:x_data, Y:y_data} ))\n",
    "logit=tf.matmul(layer2,W_3)+b_3\n",
    "h=tf.sigmoid(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,labels=Y ))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "#print(cost)\n",
    "\n",
    "#세션 초기화 -> 런을 만들고 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#학습\n",
    "\n",
    "for step in range(6000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_data , Y:y_data})\n",
    "    \n",
    "    if step%300==0 :\n",
    "        print(\"cost:{}\".format(cost_val))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#accuracy 예측값을 알아내는것 \n",
    "predict = tf.cast(h>0.5 ,dtype=tf.float32) #원래는 정수를 실수로 , 실수를 정수로 바꿔주는 cast 타입변환 1.0 0.0 으로 변환인듯?\n",
    "                                            #0.5보다 크면 1로 떨어짐\n",
    "correct = tf.equal(predict,Y) #실데이터와 예측한 것을 비교하는것 \n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct , dtype=tf.float32))\n",
    "\n",
    "\n",
    "print(\"정확도 ={}\".format(sess.run(accuracy ,feed_dict={X:x_data,Y:y_data} ) ))\n",
    "\n",
    "# W= tf.Variable([[5,-7],[5,-7]], name=\"weight\"  )\n",
    "# b= tf.Variable([[-8,3]] ,name=\"bias\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "cost: 0.4074634611606598\n",
      "cost: 0.05211479216814041\n",
      "cost: 0.10346760600805283\n",
      "cost: 0.05837707594037056\n",
      "cost: 0.058751799166202545\n",
      "cost: 0.035064999014139175\n",
      "cost: 0.027691474184393883\n",
      "cost: 0.011470389552414417\n",
      "cost: 0.01819542422890663\n",
      "cost: 0.020554106682538986\n",
      "정확도:0.9314000010490417\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "#데이터 로딩\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True) #학습용으로 나온 데이터고 그냥 읽어내면됨\n",
    "#원핫 처리없이 기본 데이터가 원핫으로 처리되어서 나옴 즉 데이터 전처리가 불필요\n",
    "\n",
    "#mnist.train.num_examples\n",
    "\n",
    "plt.imshow(mnist.train.images[0].reshape(28,28),cmap=\"Greys\",interpolation=\"nearest\"  )\n",
    "\n",
    "#interpolation 뭔뜻?  2차배열로 바꿈 원래 이미지형태로\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "#sess.run(tf.argmax(mnist.train.labels[0]) #1차 배열을 2차배열로 만들어야 argmax 됨\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2.placeholder\n",
    "#입력데이터는 이미지 데이터이고 3차원데이터인데 흑백이기때문에 2차원형태의 이미지 데이ㅓ이고 처리를 쉽게하기위해 \n",
    "#이미지 자체의 데이터를 1차원으로 표현 \n",
    "#28 * 28 이미지인데 1차원 28 * 28 => 784개의 열  \n",
    "\n",
    "\n",
    "\n",
    "X=tf.placeholder(shape=[None,784],dtype=tf.float32 )\n",
    "Y=tf.placeholder(shape=[None,10],dtype=tf.float32 )  # 0~9 까지 one hot incording 이라 10 앞에 행은 55000인데 none해줌\n",
    "\n",
    "\n",
    "#3.weight\n",
    "\n",
    "W=tf.Variable(tf.random_normal([784,256]), name=\"weight\" )\n",
    "b=tf.Variable(tf.random_normal([256]), name=\"bias\"  )\n",
    "\n",
    "layer=tf.sigmoid(tf.matmul(X,W)+b)#시그모이드해줘야함 근데 확률값을 구해야해서 소프트맥스해야하는데 레이어끼리 통과는 시그모이드로함\n",
    "\n",
    "W1=tf.Variable(tf.random_normal([256,512]), name=\"weight\" )\n",
    "b1=tf.Variable(tf.random_normal([512]), name=\"bias\"  )\n",
    "\n",
    "layer2=tf.sigmoid(tf.matmul(layer,W1)+b1)\n",
    "\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([512,10]), name=\"weight2\" )\n",
    "b2=tf.Variable(tf.random_normal([10]), name=\"bias2\"  )\n",
    "\n",
    "\n",
    "logit=tf.matmul(layer2,W2)+b2\n",
    "\n",
    "H=tf.nn.softmax(logit) #확률값으로 결과를 얻기위해서\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2( logits=logit , labels=Y ))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.3).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size=100\n",
    "train_epoch=30\n",
    "\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter=int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    avg_cost = 0\n",
    "    total_iteration = int(mnist.train._num_examples / batch_size)\n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x , batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _,cost_val=sess.run([train,cost],feed_dict={X:batch_x,Y:batch_y})\n",
    "\n",
    "        \n",
    "    if step%3 == 0:\n",
    "        print(\"cost: {}\".format(cost_val))\n",
    "        \n",
    "#acuracy\n",
    "\n",
    "\n",
    "\n",
    "predict = tf.argmax(H,1)\n",
    "correct=tf.equal(predict , tf.argmax(Y,1)) \n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(correct , dtype=tf.float32))\n",
    "\n",
    "print(\"정확도:{}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels})) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "cost: 2.3936567306518555\n",
      "cost: 2.3080689907073975\n",
      "cost: 2.362295150756836\n",
      "cost: 2.3506903648376465\n",
      "cost: 2.336737632751465\n",
      "cost: 2.3346800804138184\n",
      "cost: 2.3238463401794434\n",
      "cost: 2.3392794132232666\n",
      "cost: 2.301746368408203\n",
      "cost: 2.348781108856201\n",
      "정확도:0.09809999912977219\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sigmoid -> nn.relu 로 바꿈\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "#데이터 로딩\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True) #학습용으로 나온 데이터고 그냥 읽어내면됨\n",
    "#원핫 처리없이 기본 데이터가 원핫으로 처리되어서 나옴 즉 데이터 전처리가 불필요\n",
    "\n",
    "#mnist.train.num_examples\n",
    "\n",
    "plt.imshow(mnist.train.images[0].reshape(28,28),cmap=\"Greys\",interpolation=\"nearest\"  )\n",
    "\n",
    "#interpolation 뭔뜻?  2차배열로 바꿈 원래 이미지형태로\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "#sess.run(tf.argmax(mnist.train.labels[0]) #1차 배열을 2차배열로 만들어야 argmax 됨\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#2.placeholder\n",
    "#입력데이터는 이미지 데이터이고 3차원데이터인데 흑백이기때문에 2차원형태의 이미지 데이ㅓ이고 처리를 쉽게하기위해 \n",
    "#이미지 자체의 데이터를 1차원으로 표현 \n",
    "#28 * 28 이미지인데 1차원 28 * 28 => 784개의 열  \n",
    "\n",
    "\n",
    "\n",
    "X=tf.placeholder(shape=[None,784],dtype=tf.float32 )\n",
    "Y=tf.placeholder(shape=[None,10],dtype=tf.float32 )  # 0~9 까지 one hot incording 이라 10 앞에 행은 55000인데 none해줌\n",
    "\n",
    "\n",
    "#3.weight\n",
    "\n",
    "W=tf.Variable(tf.random_normal([784,256]), name=\"weight\" )\n",
    "b=tf.Variable(tf.random_normal([256]), name=\"bias\"  )\n",
    "\n",
    "layer=tf.nn.relu(tf.matmul(X,W)+b)#시그모이드해줘야함 근데 확률값을 구해야해서 소프트맥스해야하는데 레이어끼리 통과는 시그모이드로함\n",
    "\n",
    "W1=tf.Variable(tf.random_normal([256,512]), name=\"weight\" )\n",
    "b1=tf.Variable(tf.random_normal([512]), name=\"bias\"  )\n",
    "\n",
    "layer2=tf.nn.relu(tf.matmul(layer,W1)+b1)\n",
    "\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([512,10]), name=\"weight2\" )\n",
    "b2=tf.Variable(tf.random_normal([10]), name=\"bias2\"  )\n",
    "\n",
    "\n",
    "H=tf.matmul(layer2,W2)+b2\n",
    "\n",
    "#H=tf.nn.softmax(logit) #확률값으로 결과를 얻기위해 RELU에선 할필요없는데 해도되긴함\n",
    "\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2( logits=H , labels=Y ))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "batch_size=100\n",
    "train_epoch=30\n",
    "\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter=int(mnist.train.num_examples/batch_size)\n",
    "    \n",
    "    avg_cost = 0\n",
    "    total_iteration = int(mnist.train._num_examples / batch_size)\n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x , batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        _,cost_val=sess.run([train,cost],feed_dict={X:batch_x,Y:batch_y})\n",
    "\n",
    "        \n",
    "    if step%3 == 0:\n",
    "        print(\"cost: {}\".format(cost_val))\n",
    "        \n",
    "#acuracy\n",
    "\n",
    "\n",
    "\n",
    "predict = tf.argmax(H,1)\n",
    "correct=tf.equal(predict , tf.argmax(Y,1)) \n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(correct , dtype=tf.float32))\n",
    "\n",
    "print(\"정확도:{}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels})) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MNIST -Multinomial Classification#relu#Xavier initialization 도입 초기 W값 지정\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "#W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "W1 = tf.get_variable(\"wight1\",shape = [784,256], initializer=  tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "#W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "W2 = tf.get_variable(\"wight2\",shape = [256,512], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "#W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "W3 = tf.get_variable(\"wight3\",shape = [512,10], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "H = tf.matmul(layer2,W3) +b3\n",
    "# H = tf.nn.softmax(logit)안해도 됨\n",
    "\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = H,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "\n",
    "#정확도 출력\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-261433812bc5>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./Data/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./Data/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./Data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "0.8646591\n",
      "0.30656216\n",
      "0.2728759\n",
      "0.30054075\n",
      "0.3529044\n",
      "0.25357306\n",
      "0.22221154\n",
      "0.33714893\n",
      "0.23677036\n",
      "0.27573228\n",
      "==========\n",
      "0.9573\n"
     ]
    }
   ],
   "source": [
    "## MNIST -Multinomial Classification#relu#Xavier initialization 도입 초기 W값 지정#Drop out(overfitting 방지)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.reset_default_graph() # 텐서플로우 초기화하는것\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "#W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "keep = tf.placeholder(dtype = tf.float32) #입력 값으로 사용하겠다. 상수로 박지 않고. \n",
    "W1 = tf.get_variable(\"wight1\",shape = [784,256], initializer=  tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep) #keepprob 유지할 확률 0.5~0.7\n",
    "#W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "W2 = tf.get_variable(\"wight2\",shape = [256,512], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep)\n",
    "#W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "W3 = tf.get_variable(\"wight3\",shape = [512,10], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "H = tf.matmul(layer2,W3) +b3\n",
    "# H = tf.nn.softmax(logit)안해도 됨\n",
    "\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = H,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y,keep:0.7})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "\n",
    "#정확도 출력\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels, keep:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "data_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
