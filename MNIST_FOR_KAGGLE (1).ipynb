{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 학습완료!!\n",
      "Model 1 학습완료!!\n",
      "Model 2 학습완료!!\n",
      "Model 3 학습완료!!\n",
      "Model 4 학습완료!!\n",
      "Model 5 학습완료!!\n",
      "Model 6 학습완료!!\n",
      "Model 7 학습완료!!\n",
      "Model 8 학습완료!!\n",
      "Model 9 학습완료!!\n",
      "Model 10 학습완료!!\n",
      "Model 11 학습완료!!\n",
      "Model 12 학습완료!!\n",
      "Model 13 학습완료!!\n",
      "Model 14 학습완료!!\n",
      "Model 15 학습완료!!\n",
      "Model 16 학습완료!!\n",
      "Model 17 학습완료!!\n",
      "Model 18 학습완료!!\n",
      "Model 19 학습완료!!\n",
      "Model0 - Accuracy : 0.9992857142857143\n",
      "Model1 - Accuracy : 0.9994444444444445\n",
      "Model2 - Accuracy : 0.9976190476190476\n",
      "Model3 - Accuracy : 0.995\n",
      "Model4 - Accuracy : 0.9992063492063492\n",
      "Model5 - Accuracy : 0.9968253968253968\n",
      "Model6 - Accuracy : 0.9987301587301587\n",
      "Model7 - Accuracy : 0.9984126984126984\n",
      "Model8 - Accuracy : 0.9985714285714286\n",
      "Model9 - Accuracy : 0.9938095238095238\n",
      "Model10 - Accuracy : 0.996031746031746\n",
      "Model11 - Accuracy : 0.9994444444444445\n",
      "Model12 - Accuracy : 0.9992857142857143\n",
      "Model13 - Accuracy : 0.9974603174603175\n",
      "Model14 - Accuracy : 0.9981746031746032\n",
      "Model15 - Accuracy : 0.9976984126984128\n",
      "Model16 - Accuracy : 0.9992063492063492\n",
      "Model17 - Accuracy : 0.9992857142857143\n",
      "Model18 - Accuracy : 0.9992857142857143\n",
      "Model19 - Accuracy : 0.9971428571428571\n",
      "Accuracy : 1.0\n",
      "Wall time: 23min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### MNIST(Kaggle) with CNN ( class를 이용한 ensemble )\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "  \n",
    "## reset tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "### Class Definition\n",
    "class CnnModel:\n",
    "    \n",
    "    def __init__(self,sess,name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self.build_net()        \n",
    "        \n",
    "    def build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "            self.Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "            self.keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "            ## Convolution layer\n",
    "            X_img = tf.reshape(self.X, shape=[-1,28,28,1])\n",
    "            \n",
    "#             #     Filter를 생성\n",
    "#             W1 = tf.Variable(tf.random_normal([3,3,1,32], stddev=0.01))\n",
    "#             #     Convolution\n",
    "#             L1 = tf.nn.conv2d(X_img, W1, strides=[1,1,1,1], padding=\"SAME\")\n",
    "#             #     ReLU\n",
    "#             L1 = tf.nn.relu(L1)\n",
    "#             #     MAX Pooling\n",
    "#             L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], \n",
    "#                                 strides=[1,2,2,1], padding=\"SAME\")\n",
    "            \n",
    "            L1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3,3], \n",
    "                                  padding=\"SAME\", strides=1, activation=tf.nn.relu)\n",
    "            L1 = tf.layers.max_pooling2d(inputs=L1, pool_size=[2,2], \n",
    "                                         padding=\"SAME\", strides=2)\n",
    "            L1 = tf.layers.dropout(inputs=L1, rate=0.3)\n",
    "            \n",
    "            \n",
    "            L2 = tf.layers.conv2d(inputs=L1, filters=64, kernel_size=[3,3], \n",
    "                                  padding=\"SAME\", strides=1, activation=tf.nn.relu)\n",
    "            L2 = tf.layers.max_pooling2d(inputs=L2, pool_size=[2,2], \n",
    "                                         padding=\"SAME\", strides=2)\n",
    "            L2 = tf.layers.dropout(inputs=L2, rate=0.3)\n",
    "            \n",
    "            \n",
    "            L3 = tf.layers.conv2d(inputs=L2, filters=128, kernel_size=[3,3], \n",
    "                                  padding=\"SAME\", strides=1, activation=tf.nn.relu)\n",
    "            L3 = tf.layers.max_pooling2d(inputs=L3, pool_size=[2,2], \n",
    "                                         padding=\"SAME\", strides=2)\n",
    "            L3 = tf.layers.dropout(inputs=L3, rate=0.3)\n",
    "                                    \n",
    "            L3 = tf.reshape(L3, shape=[-1,4*4*128])\n",
    "            \n",
    "#             W1 = tf.get_variable(\"weight1\", shape=[4*4*128,256],\n",
    "#                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "#             b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "#             _layer1 = tf.nn.relu(tf.matmul(L2,W1) + b1)\n",
    "#             layer1 = tf.layers.dropout(_layer1, rate=self.keep_prob)\n",
    "            \n",
    "            ## dense layer\n",
    "            dense1 = tf.layers.dense(inputs=L3, \n",
    "                                     units=128, \n",
    "                                     activation=tf.nn.relu)\n",
    "            dense1 = tf.layers.dropout(inputs=dense1, \n",
    "                                       rate=self.keep_prob)\n",
    "\n",
    "            \n",
    "            dense2 = tf.layers.dense(inputs=dense1, units=256, activation=tf.nn.relu)\n",
    "            dense2 = tf.layers.dropout(inputs=dense2, rate=self.keep_prob)\n",
    "\n",
    "            dense3 = tf.layers.dense(inputs=dense2, units=128, activation=tf.nn.relu)\n",
    "            dense3 = tf.layers.dropout(inputs=dense3, rate=self.keep_prob)\n",
    "\n",
    "            dense4 = tf.layers.dense(inputs=dense3, units=512, activation=tf.nn.relu)\n",
    "            dense4 = tf.layers.dropout(inputs=dense4, rate=self.keep_prob)\n",
    "\n",
    "            dense5 = tf.layers.dense(inputs=dense4, units=1024, activation=tf.nn.relu)\n",
    "            dense5 = tf.layers.dropout(inputs=dense5, rate=self.keep_prob)\n",
    "            \n",
    "#             self.H = tf.matmul(layer2, W3) + b3\n",
    "            self.H = tf.layers.dense(inputs=dense5, units=10)\n",
    "            # FC Layer ( Neural Network )\n",
    "#             W1 = tf.get_variable(\"weight1\", shape=[7*7*64,256],\n",
    "#                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "#             b1 = tf.Variable(tf.random_normal([256]), name=\"bias1\")\n",
    "#             _layer1 = tf.nn.relu(tf.matmul(L2,W1) + b1)\n",
    "#             layer1 = tf.layers.dropout(_layer1, rate=self.keep_prob)\n",
    "\n",
    "#             W2 = tf.get_variable(\"weight2\", shape=[256,256],\n",
    "#                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "#             b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "#             _layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)\n",
    "#             layer2 = tf.layers.dropout(_layer2, rate=self.keep_prob)\n",
    "\n",
    "#             W3 = tf.get_variable(\"weight3\", shape=[256,10],\n",
    "#                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "#             b3 = tf.Variable(tf.random_normal([10]), name=\"bias3\")\n",
    "\n",
    "#             self.H = tf.matmul(layer2, W3) + b3\n",
    "        \n",
    "#         self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.H, labels=self.Y))\n",
    "#         self.train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.cost)\n",
    "\n",
    "        self.cost = tf.losses.softmax_cross_entropy(self.Y, \n",
    "                                                    self.H)\n",
    "        self.train = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(self.cost)\n",
    "\n",
    "        self.predict = tf.argmax(self.H,1)\n",
    "        self.correct = tf.equal(self.predict, tf.argmax(self.Y,1))\n",
    "        self.correct_count = tf.reduce_sum(tf.cast(self.correct, dtype=tf.float32))\n",
    "        \n",
    "    def train_net(self,train_x, train_y, rate):\n",
    "        _, cost_val = self.sess.run([self.train, self.cost], feed_dict={self.X:train_x,\n",
    "                                                                        self.Y:train_y,\n",
    "                                                                        self.keep_prob:rate})\n",
    "        \n",
    "    \n",
    "    def get_prediction(self,test_x, rate):\n",
    "        H_val = self.sess.run(self.H, feed_dict={self.X:test_x,\n",
    "                                                          self.keep_prob:rate})\n",
    "        return H_val\n",
    "    \n",
    "    def get_accuracy(self,test_x,test_y,rate):\n",
    "        return self.sess.run(self.correct_count, feed_dict={self.X:test_x,\n",
    "                                                            self.Y:test_y,\n",
    "                                                            self.keep_prob:rate})\n",
    "        \n",
    "## 1. Data Loading\n",
    "############# MNIST DATA\n",
    "# mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "\n",
    "# train_x_data = mnist.train.images\n",
    "# test_x_data = mnist.test.images\n",
    "\n",
    "# sess = tf.Session()\n",
    "\n",
    "# train_y_data = mnist.train.labels\n",
    "# test_y_data = mnist.test.labels\n",
    "\n",
    "############# Kaggle Data ( accuracy 측정을 위한 7:3 분할 )\n",
    "# data = pd.read_csv(\"./data/kaggle_mnist/train.csv\", sep=\",\")\n",
    "\n",
    "# data_x = data.drop(\"label\", axis=1, inplace=False)\n",
    "# data_y = data[\"label\"]\n",
    "\n",
    "# sess = tf.Session()\n",
    "# num_of_train = int(data.shape[0] * 0.7) # 70%\n",
    "\n",
    "# train_x_data = data_x.loc[:num_of_train,:].values\n",
    "# test_x_data = data_x.loc[num_of_train+1:,:].values\n",
    "\n",
    "# train_y_data = sess.run(tf.one_hot(data_y[:num_of_train+1].values,10))\n",
    "# test_y_data = sess.run(tf.one_hot(data_y[num_of_train+1:].values, 10))\n",
    "\n",
    "############# Kaggle Data ( train data를 모두 이용해서 학습 )\n",
    "data = pd.read_csv(\"./data/kaggle_mnist/train.csv\", sep=\",\")\n",
    "\n",
    "data_x = data.drop(\"label\", axis=1, inplace=False)\n",
    "data_y = data[\"label\"]\n",
    "\n",
    "sess = tf.Session()\n",
    "num_of_train = int(data.shape[0] * 0.7) # 70%\n",
    "\n",
    "train_x_data = data_x.loc[:,:].values\n",
    "test_x_data = data_x.loc[num_of_train+1:,:].values\n",
    "\n",
    "train_y_data = sess.run(tf.one_hot(data_y[:].values,10))\n",
    "test_y_data = sess.run(tf.one_hot(data_y[num_of_train+1:].values, 10))\n",
    "\n",
    "\n",
    "## 3. Model 객체 생성\n",
    "num_of_models = 20\n",
    "\n",
    "models = [CnnModel(sess,\"model\"+ str(x)) for x in range(num_of_models)]\n",
    "\n",
    "## 4. 초기화\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "## 5. Model 학습\n",
    "num_of_epoch = 20\n",
    "batch_size = 100\n",
    "keep_rate = 0.5\n",
    "\n",
    "for model_idx in range(num_of_models):\n",
    "    for step in range(num_of_epoch):\n",
    "        num_of_iter = int(train_x_data.shape[0] / batch_size)\n",
    "        idx = 0\n",
    "        \n",
    "        for i in range(num_of_iter):\n",
    "            batch_x = train_x_data[idx:idx+batch_size,:]\n",
    "            batch_y = train_y_data[idx:idx+batch_size,:]\n",
    "            idx = idx + batch_size\n",
    "            models[model_idx].train_net(batch_x, batch_y,keep_rate)\n",
    "            \n",
    "    print(\"Model {} 학습완료!!\".format(model_idx))\n",
    "\n",
    "## 6. 각 model의 Accuracy 측정\n",
    "keep_rate = 1\n",
    "num_of_iter = int(test_x_data.shape[0] / batch_size)\n",
    "\n",
    "for model_idx in range(num_of_models):\n",
    "    correct_count = 0\n",
    "    idx = 0\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x = test_x_data[idx:idx+batch_size,:]\n",
    "        batch_y = test_y_data[idx:idx+batch_size,:]\n",
    "        idx = idx + batch_size\n",
    "        count = models[model_idx].get_accuracy(batch_x, batch_y, keep_rate)\n",
    "        correct_count += count\n",
    "    print(\"Model{} - Accuracy : {}\".format(model_idx,correct_count / test_x_data.shape[0]))    \n",
    "\n",
    "## 7. ensemble의 Accuracy 측정\n",
    "keep_rate = 1\n",
    "idx = 0\n",
    "correct_sum = 0\n",
    "num_of_iter = int(test_x_data.shape[0] / batch_size)\n",
    "\n",
    "l_correct_count = 0\n",
    "\n",
    "for i in range(num_of_iter):\n",
    "    batch_x = test_x_data[idx:idx+batch_size,:]\n",
    "    batch_y = test_y_data[idx:idx+batch_size,:]\n",
    "    idx = idx + batch_size\n",
    "    \n",
    "    prediction = np.zeros([batch_size,10])\n",
    "    \n",
    "    for model_idx in range(num_of_models):\n",
    "        p = models[model_idx].get_prediction(batch_x,keep_rate)\n",
    "        prediction += p\n",
    "    ## prediction을 구했어요\n",
    "    \n",
    "    l_predict = tf.argmax(prediction,1)\n",
    "    l_correct = tf.equal(l_predict, tf.argmax(batch_y,1))\n",
    "    l_count = tf.reduce_sum(tf.cast(l_correct, dtype=tf.float32))\n",
    "    \n",
    "    l_count_result = sess.run(l_count)\n",
    "    l_correct_count += l_count_result\n",
    "    \n",
    "print(\"Accuracy : {}\".format(l_correct_count / test_x_data.shape[0]))\n",
    "\n",
    "## 8. 결과 도출\n",
    "\n",
    "data = pd.read_csv(\"./data/kaggle_mnist/test.csv\", sep=\",\")\n",
    "\n",
    "test_x_data = data.values\n",
    "\n",
    "keep_rate = 1\n",
    "idx = 0\n",
    "correct_sum = 0\n",
    "num_of_iter = int(test_x_data.shape[0] / batch_size)\n",
    "\n",
    "l_correct_count = 0\n",
    "result = []\n",
    "\n",
    "for i in range(num_of_iter):\n",
    "    batch_x = test_x_data[idx:idx+batch_size,:]\n",
    "    batch_y = test_y_data[idx:idx+batch_size,:]\n",
    "    idx = idx + batch_size\n",
    "    \n",
    "    prediction = np.zeros([batch_size,10])\n",
    "    \n",
    "    for model_idx in range(num_of_models):\n",
    "        p = models[model_idx].get_prediction(batch_x,keep_rate)\n",
    "        prediction += p\n",
    "#     ## prediction을 구했어요\n",
    "    \n",
    "    l_predict = tf.argmax(prediction,1)\n",
    "    tmp = sess.run(l_predict)\n",
    "    result.extend(tmp)\n",
    "\n",
    "df1 = pd.DataFrame([x+1 for x in range(test_x_data.shape[0])], columns=[\"ImageId\"])\n",
    "df2 = pd.DataFrame(result, columns=[\"Label\"])\n",
    "df3 = df1.join(df2)\n",
    "df3.to_csv(\"./data/kaggle_mnist/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
