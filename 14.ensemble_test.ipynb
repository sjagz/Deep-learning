{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FailedPreconditionError",
     "evalue": "Failed to allocate scratch buffer for device 0\n\t [[{{node _SOURCE}} = NoOp[]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Failed to allocate scratch buffer for device 0\n\t [[{{node _SOURCE}} = NoOp[]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a24f672338f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;31m###세션 초기화\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;31m###세션 초기화 끝\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Failed to allocate scratch buffer for device 0\n\t [[{{node _SOURCE}} = NoOp[]()]]"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "## reset tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class CNNModel: \n",
    "    \n",
    "    def __init__(self,sess,name):     \n",
    "\n",
    "        self.sess=sess\n",
    "        self.name=name \n",
    "        \n",
    "    def build_graph(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.X=tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "            self.Y=tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "            self.keep_rate=tf.placeholder(dtype=tf.float32) \n",
    "            \n",
    "            X_img = tf.reshape(self.X, shape=[-1,28,28,1])\n",
    "\n",
    "            W1=tf.Variable(tf.random_normal([5,5,1,32],stddev=0.01))\n",
    "\n",
    "            L1 = tf.nn.conv2d(X_img,W1,strides=[1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "            L1 = tf.nn.relu(L1)\n",
    "            p1 = tf.nn.max_pool(L1 , ksize=[1,2,2,1] , strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "            L2=tf.layers.conv2d(inputs=p1, filters=64, kernel_size=[5,5] , padding=\"SAME\", strides=1 ,activation=tf.nn.relu)\n",
    "\n",
    "            L2=tf.layers.max_pooling2d(inputs=L2 , pool_size=[2,2], padding=\"SAME\" , strides=2)\n",
    "\n",
    "            L2=tf.reshape(L2,shape=[-1,7*7*64])\n",
    "\n",
    "            W2=tf.get_variable(\"weight2\" , shape=[7*7*64,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2=tf.Variable(tf.random_normal([256]),name=\"bias2\" )\n",
    "\n",
    "            _layer1 = tf.nn.relu(tf.matmul(L2,W2)+b2 )\n",
    "            layer1 = tf.nn.dropout(_layer1 , keep_prob=self.keep_rate)\n",
    "\n",
    "\n",
    "            W3=tf.get_variable(\"weight3\" , shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b3=tf.Variable(tf.random_normal([256]),name=\"bias3\" )\n",
    "\n",
    "            _layer2 = tf.nn.relu(tf.matmul(layer1,W3)+b3 )\n",
    "            layer2 = tf.nn.dropout(_layer2 , keep_prob=self.keep_rate)\n",
    "\n",
    "\n",
    "            W4=tf.get_variable(\"weight4\" , shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b4=tf.Variable(tf.random_normal([10]),name=\"bias4\" )\n",
    "\n",
    "            self.H = tf.matmul(layer2,W4)+b4\n",
    "\n",
    "            self.cost= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.H , labels=self.Y))\n",
    "\n",
    "            self.train = tf.train.AdamOptimizer(learning_rate=0.000833).minimize(self.cost)\n",
    "\n",
    "            self.predict = tf.argmax(self.H,1)\n",
    "            self.correct=tf.equal(self.predict , tf.argmax(self.Y,1)) \n",
    "\n",
    "            self.accuracy=tf.reduce_sum(tf.cast(self.correct , dtype=tf.float32))\n",
    "\n",
    "    \n",
    "    def train_model(self,x,y,keep):\n",
    "        \n",
    "        _,cost_val=self.sess.run([self.train,self.cost], feed_dict={self.X:x , self.Y:y , self.keep_rate:keep})\n",
    "        \n",
    "            \n",
    "    \n",
    "    def H_value_sum(self,x,y):\n",
    "        \n",
    "        n=0\n",
    "        n1=100\n",
    "        n2=100\n",
    "        H_zero = np.zeros((100,10),dtype=np.float32)\n",
    "        dataindex_len=len(x_data)\n",
    "        dataindex_len=int(dataindex_len/100)\n",
    "        for num in range(10):\n",
    "            ### 모델 10개\n",
    "            model_list = [\"m0\",\"m1\"]#,\"m2\",\"m3\",\"m4\",\"m5\",\"m6\",\"m7\",\"m8\",\"m9\"]\n",
    "            \n",
    "            for k in model_list:\n",
    "                batch_x = x[n:n2]\n",
    "                batch_y = y[n:n2]\n",
    "                H_val=(self.sess.run(self.H, feed_dict={self.X:batch_x , self.keep_rate:0.5 }))\n",
    "\n",
    "                H_zero=H_zero+H_val\n",
    "            \n",
    "            self.H=H_zero/10\n",
    "            print(self.H)\n",
    "\n",
    "            n=n+n1\n",
    "            n2=n2+n1\n",
    "        return(self.H)\n",
    "\n",
    "### x_data , y_data -> dataframe 형태\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train = pd.read_csv(\"./data/mnist2/train.csv\")\n",
    "test = pd.read_csv(\"./data/mnist2/test.csv\")\n",
    "\n",
    "\n",
    "x_data=train.drop(\"label\", axis=1, inplace=False)\n",
    "train2=train[\"label\"]\n",
    "\n",
    " \n",
    "df1=pd.DataFrame()\n",
    "arr = np.zeros((10,10), dtype=np.int32)\n",
    "df1=arr\n",
    "df2=pd.DataFrame()\n",
    "for n in range(0,10):\n",
    "    df1[n,n]=1\n",
    "    df2[n]=np.arange(10)\n",
    "\n",
    "tr2=int(len(train2))\n",
    "for n in range(0,tr2):\n",
    "    df2[n]=df1[train2[n]]\n",
    "y_data=pd.DataFrame()\n",
    "y_data=df2.transpose()\n",
    "\n",
    "\n",
    "x_data=x_data.values\n",
    "y_data=y_data.values\n",
    "### x_data , y_data -> dataframe 형태 -> 넘파이 어레이가 공통적으로 쓰임\n",
    "\n",
    "###세션생성\n",
    "sess=tf.Session()\n",
    "\n",
    "###세션생성\n",
    "\n",
    "###모델 생성\n",
    "count_of_models = 2\n",
    "models = [CNNModel(sess,\"model\"+ str(x)) for x in range(count_of_models)]\n",
    "##모델 생성 끝\n",
    "\n",
    "###세션 초기화 \n",
    "sess.run(tf.global_variables_initializer())\n",
    "###세션 초기화 끝\n",
    "\n",
    "\n",
    "###학습\n",
    "for num in range(count_of_models):\n",
    "    models[num].build_graph()\n",
    "    \n",
    "    print(models[num])\n",
    "    \n",
    "    start_roof=0\n",
    "    increase_roof=100\n",
    "    finish_roof=100\n",
    "    batch_size=100\n",
    "    dataindex_len=len(x_data)\n",
    "    traindataindex_len=int(dataindex_len/batch_size)\n",
    "    \n",
    "    print(traindataindex_len)\n",
    "    H_zero = np.zeros((100,10),dtype=np.float32)\n",
    "    n=0\n",
    "    n1=100\n",
    "    n2=100\n",
    "    keep_rate=0.5\n",
    "    \n",
    "    for step in range(10):\n",
    "\n",
    "        start_roof=0\n",
    "        increase_roof=100\n",
    "        finish_roof=100\n",
    "\n",
    "        for i in range(traindataindex_len):\n",
    "            batch_x = x_data[start_roof:finish_roof]\n",
    "            batch_y = y_data[start_roof:finish_roof]\n",
    "            models[num].train_model(batch_x , batch_y , keep_rate)\n",
    "            start_roof=start_roof+increase_roof\n",
    "            finish_roof=finish_roof+increase_roof\n",
    "\n",
    "        print(\"cost : {}\".format(cost_val))\n",
    "##학습끝\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.CNNModel object at 0x000000000F9B2978>, <__main__.CNNModel object at 0x000000000F9B2BA8>]\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "## reset tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class CNNModel: \n",
    "    \n",
    "    def __init__(self,sess,name):     \n",
    "\n",
    "        self.sess=sess\n",
    "        self.name=name \n",
    "        \n",
    "    def build_graph(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.X=tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "            self.Y=tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "            self.keep_rate=tf.placeholder(dtype=tf.float32) \n",
    "            \n",
    "            X_img = tf.reshape(self.X, shape=[-1,28,28,1])\n",
    "\n",
    "            W1=tf.Variable(tf.random_normal([5,5,1,32],stddev=0.01))\n",
    "\n",
    "            L1 = tf.nn.conv2d(X_img,W1,strides=[1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "            L1 = tf.nn.relu(L1)\n",
    "            p1 = tf.nn.max_pool(L1 , ksize=[1,2,2,1] , strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "            L2=tf.layers.conv2d(inputs=p1, filters=64, kernel_size=[5,5] , padding=\"SAME\", strides=1 ,activation=tf.nn.relu)\n",
    "\n",
    "            L2=tf.layers.max_pooling2d(inputs=L2 , pool_size=[2,2], padding=\"SAME\" , strides=2)\n",
    "\n",
    "            L2=tf.reshape(L2,shape=[-1,7*7*64])\n",
    "\n",
    "            W2=tf.get_variable(\"weight2\" , shape=[7*7*64,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2=tf.Variable(tf.random_normal([256]),name=\"bias2\" )\n",
    "\n",
    "            _layer1 = tf.nn.relu(tf.matmul(L2,W2)+b2 )\n",
    "            layer1 = tf.nn.dropout(_layer1 , keep_prob=self.keep_rate)\n",
    "\n",
    "\n",
    "            W3=tf.get_variable(\"weight3\" , shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b3=tf.Variable(tf.random_normal([256]),name=\"bias3\" )\n",
    "\n",
    "            _layer2 = tf.nn.relu(tf.matmul(layer1,W3)+b3 )\n",
    "            layer2 = tf.nn.dropout(_layer2 , keep_prob=self.keep_rate)\n",
    "\n",
    "\n",
    "            W4=tf.get_variable(\"weight4\" , shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b4=tf.Variable(tf.random_normal([10]),name=\"bias4\" )\n",
    "\n",
    "            self.H = tf.matmul(layer2,W4)+b4\n",
    "\n",
    "            self.cost= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.H , labels=self.Y))\n",
    "\n",
    "            self.train = tf.train.AdamOptimizer(learning_rate=0.000833).minimize(self.cost)\n",
    "\n",
    "            self.predict = tf.argmax(self.H,1)\n",
    "            self.correct=tf.equal(self.predict , tf.argmax(self.Y,1)) \n",
    "\n",
    "            self.accuracy=tf.reduce_sum(tf.cast(self.correct , dtype=tf.float32))\n",
    "\n",
    "    \n",
    "    def train_model(self,x,y,keep):\n",
    "        \n",
    "        _,cost_val=self.sess.run([self.train,self.cost], feed_dict={self.X:x , self.Y:y , self.keep_rate:keep})\n",
    "        \n",
    "            \n",
    "    \n",
    "    def H_value_sum(self,x,y):\n",
    "        \n",
    "        n=0\n",
    "        n1=100\n",
    "        n2=100\n",
    "        H_zero = np.zeros((100,10),dtype=np.float32)\n",
    "        dataindex_len=len(x_data)\n",
    "        dataindex_len=int(dataindex_len/100)\n",
    "        for num in range(10):\n",
    "            ### 모델 10개\n",
    "            model_list = [\"m0\",\"m1\"]#,\"m2\",\"m3\",\"m4\",\"m5\",\"m6\",\"m7\",\"m8\",\"m9\"]\n",
    "            \n",
    "            for k in model_list:\n",
    "                batch_x = x[n:n2]\n",
    "                batch_y = y[n:n2]\n",
    "                H_val=(self.sess.run(self.H, feed_dict={self.X:batch_x , self.keep_rate:0.5 }))\n",
    "\n",
    "                H_zero=H_zero+H_val\n",
    "            \n",
    "            self.H=H_zero/10\n",
    "            print(self.H)\n",
    "\n",
    "            n=n+n1\n",
    "            n2=n2+n1\n",
    "        return(self.H)\n",
    "\n",
    "### x_data , y_data -> dataframe 형태\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train = pd.read_csv(\"./data/mnist2/train.csv\")\n",
    "test = pd.read_csv(\"./data/mnist2/test.csv\")\n",
    "\n",
    "\n",
    "x_data=train.drop(\"label\", axis=1, inplace=False)\n",
    "train2=train[\"label\"]\n",
    "\n",
    " \n",
    "df1=pd.DataFrame()\n",
    "df2=pd.DataFrame()\n",
    "arr = np.zeros((10,10), dtype=np.int32)\n",
    "df1=arr\n",
    "for n in range(0,10):\n",
    "    df1[n,n]=1\n",
    "    df2[n]=np.arange(10)\n",
    "\n",
    "tr2=int(len(train2))\n",
    "for n in range(0,tr2):\n",
    "    df2[n]=df1[train2[n]]\n",
    "y_data=pd.DataFrame()\n",
    "y_data=df2.transpose()\n",
    "\n",
    "\n",
    "x_data=x_data.values\n",
    "y_data=y_data.values\n",
    "### x_data , y_data -> dataframe 형태 -> 넘파이 어레이가 공통적으로 쓰임\n",
    "\n",
    "###세션생성\n",
    "sess=tf.Session()\n",
    "\n",
    "###세션생성\n",
    "\n",
    "###모델 생성\n",
    "count_of_models = 2\n",
    "models = [CNNModel(sess,\"model\"+ str(x)) for x in range(count_of_models)]\n",
    "##모델 생성 끝\n",
    "\n",
    "print(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "===============\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[3136,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model9/model9/weight2/Adam_1/Assign (defined at <ipython-input-3-e603389f9ea9>:110)  = Assign[T=DT_FLOAT, _class=[\"loc:@model9/weight2/Assign\"], _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model9/model9/weight2/Adam_1, model0/model0/weight2/Adam/Initializer/zeros)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'model9/model9/weight2/Adam_1/Assign', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3209, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-e603389f9ea9>\", line 128, in <module>\n    models = [CNNModel(sess,\"model\"+ str(x)) for x in range(count_of_models)]\n  File \"<ipython-input-3-e603389f9ea9>\", line 128, in <listcomp>\n    models = [CNNModel(sess,\"model\"+ str(x)) for x in range(count_of_models)]\n  File \"<ipython-input-3-e603389f9ea9>\", line 67, in __init__\n    self.build_graph()            # 해주고 안해주고 차이가 뭐지?\n  File \"<ipython-input-3-e603389f9ea9>\", line 110, in build_graph\n    self.train = tf.train.AdamOptimizer(learning_rate=0.000833).minimize(self.cost)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 410, in minimize\n    name=name)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 593, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 136, in _create_slots\n    self._zeros_slot(v, \"v\", self._name)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 1139, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 183, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 157, in create_slot_with_initializer\n    dtype)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 65, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1487, in get_variable\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1237, in get_variable\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 540, in get_variable\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 492, in _true_getter\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 922, in _get_single_variable\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 183, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 146, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 125, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2444, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 187, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1329, in __init__\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1481, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 221, in assign\n    validate_shape=validate_shape)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 61, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3136,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model9/model9/weight2/Adam_1/Assign (defined at <ipython-input-3-e603389f9ea9>:110)  = Assign[T=DT_FLOAT, _class=[\"loc:@model9/weight2/Assign\"], _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model9/model9/weight2/Adam_1, model0/model0/weight2/Adam/Initializer/zeros)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3136,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node model9/model9/weight2/Adam_1/Assign}} = Assign[T=DT_FLOAT, _class=[\"loc:@model9/weight2/Assign\"], _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model9/model9/weight2/Adam_1, model0/model0/weight2/Adam/Initializer/zeros)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-e603389f9ea9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[3136,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model9/model9/weight2/Adam_1/Assign (defined at <ipython-input-3-e603389f9ea9>:110)  = Assign[T=DT_FLOAT, _class=[\"loc:@model9/weight2/Assign\"], _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model9/model9/weight2/Adam_1, model0/model0/weight2/Adam/Initializer/zeros)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'model9/model9/weight2/Adam_1/Assign', defined at:\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2843, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2869, in _run_cell\n    return runner(coro)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3044, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3209, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3291, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-e603389f9ea9>\", line 128, in <module>\n    models = [CNNModel(sess,\"model\"+ str(x)) for x in range(count_of_models)]\n  File \"<ipython-input-3-e603389f9ea9>\", line 128, in <listcomp>\n    models = [CNNModel(sess,\"model\"+ str(x)) for x in range(count_of_models)]\n  File \"<ipython-input-3-e603389f9ea9>\", line 67, in __init__\n    self.build_graph()            # 해주고 안해주고 차이가 뭐지?\n  File \"<ipython-input-3-e603389f9ea9>\", line 110, in build_graph\n    self.train = tf.train.AdamOptimizer(learning_rate=0.000833).minimize(self.cost)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 410, in minimize\n    name=name)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 593, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 136, in _create_slots\n    self._zeros_slot(v, \"v\", self._name)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 1139, in _zeros_slot\n    new_slot_variable = slot_creator.create_zeros_slot(var, op_name)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 183, in create_zeros_slot\n    colocate_with_primary=colocate_with_primary)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 157, in create_slot_with_initializer\n    dtype)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 65, in _create_slot_var\n    validate_shape=validate_shape)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1487, in get_variable\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 1237, in get_variable\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 540, in get_variable\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 492, in _true_getter\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 922, in _get_single_variable\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 183, in __call__\n    return cls._variable_v1_call(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 146, in _variable_v1_call\n    aggregation=aggregation)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 125, in <lambda>\n    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\", line 2444, in default_variable_creator\n    expected_shape=expected_shape, import_scope=import_scope)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 187, in __call__\n    return super(VariableMetaclass, cls).__call__(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1329, in __init__\n    constraint=constraint)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\variables.py\", line 1481, in _init_from_args\n    validate_shape=validate_shape).op\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\state_ops.py\", line 221, in assign\n    validate_shape=validate_shape)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_state_ops.py\", line 61, in assign\n    use_locking=use_locking, name=name)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[3136,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model9/model9/weight2/Adam_1/Assign (defined at <ipython-input-3-e603389f9ea9>:110)  = Assign[T=DT_FLOAT, _class=[\"loc:@model9/weight2/Assign\"], _grappler_relax_allocator_constraints=true, use_locking=true, validate_shape=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model9/model9/weight2/Adam_1, model0/model0/weight2/Adam/Initializer/zeros)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "## reset tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "### x_data , y_data -> dataframe 형태\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train = pd.read_csv(\"./data/mnist2/train.csv\")\n",
    "test = pd.read_csv(\"./data/mnist2/test.csv\")\n",
    "\n",
    "\n",
    "x_data=train.drop(\"label\", axis=1, inplace=False)\n",
    "train2=train[\"label\"]\n",
    "\n",
    " \n",
    "df1=pd.DataFrame()\n",
    "arr = np.zeros((10,10), dtype=np.int32)\n",
    "df1=arr\n",
    "df2=pd.DataFrame()\n",
    "for n in range(0,10):\n",
    "    df1[n,n]=1\n",
    "    df2[n]=np.arange(10)\n",
    "\n",
    "tr2=int(len(train2))\n",
    "for n in range(0,tr2):\n",
    "    df2[n]=df1[train2[n]]\n",
    "y_data=pd.DataFrame()\n",
    "y_data=df2.transpose()\n",
    "\n",
    "number=int(len(x_data)*2/3)\n",
    "\n",
    "xtrain_data=x_data[0:number].values\n",
    "xtext_data=x_data[number:-1].values\n",
    "\n",
    "ytrain_data=y_data[0:number].values\n",
    "ytest_data=y_data[number:-1].values\n",
    "print(\"===============\")\n",
    "print(xtrain_data)\n",
    "x_data=x_data.values\n",
    "y_data=y_data.values\n",
    "\n",
    "\n",
    "print(\"===============\")\n",
    "\n",
    "### x_data , y_data -> dataframe 형태 -> 넘파이 어레이가 공통적으로 쓰임\n",
    "# display(x_data)\n",
    "# print(\"============\")\n",
    "# display(y_data) #성공\n",
    "\n",
    "#클레스 생성\n",
    "\n",
    "class CNNModel: \n",
    "    \n",
    "    def __init__(self,sess,name):     \n",
    "\n",
    "        self.sess=sess\n",
    "        self.name=name \n",
    "        self.build_graph()            # 해주고 안해주고 차이가 뭐지?\n",
    "    def build_graph(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.X=tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "            self.Y=tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "            self.keep_rate=tf.placeholder(dtype=tf.float32) \n",
    "            \n",
    "            X_img = tf.reshape(self.X, shape=[-1,28,28,1])\n",
    "\n",
    "            W1=tf.Variable(tf.random_normal([5,5,1,32],stddev=0.01))\n",
    "\n",
    "            L1 = tf.nn.conv2d(X_img,W1,strides=[1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "            L1 = tf.nn.relu(L1)\n",
    "            p1 = tf.nn.max_pool(L1 , ksize=[1,2,2,1] , strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "            L2=tf.layers.conv2d(inputs=p1, filters=64, kernel_size=[5,5] , padding=\"SAME\", strides=1 ,activation=tf.nn.relu)\n",
    "\n",
    "            L2=tf.layers.max_pooling2d(inputs=L2 , pool_size=[2,2], padding=\"SAME\" , strides=2)\n",
    "\n",
    "            L2=tf.reshape(L2,shape=[-1,7*7*64])\n",
    "\n",
    "            W2=tf.get_variable(\"weight2\" , shape=[7*7*64,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2=tf.Variable(tf.random_normal([256]),name=\"bias2\" )\n",
    "\n",
    "            _layer1 = tf.nn.relu(tf.matmul(L2,W2)+b2 )\n",
    "            layer1 = tf.nn.dropout(_layer1 , keep_prob=self.keep_rate)\n",
    "\n",
    "\n",
    "            W3=tf.get_variable(\"weight3\" , shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b3=tf.Variable(tf.random_normal([256]),name=\"bias3\" )\n",
    "\n",
    "            _layer2 = tf.nn.relu(tf.matmul(layer1,W3)+b3 )\n",
    "            layer2 = tf.nn.dropout(_layer2 , keep_prob=self.keep_rate)\n",
    "\n",
    "\n",
    "            W4=tf.get_variable(\"weight4\" , shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b4=tf.Variable(tf.random_normal([10]),name=\"bias4\" )\n",
    "\n",
    "            self.H = tf.matmul(layer2,W4)+b4\n",
    "\n",
    "            self.cost= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.H , labels=self.Y))\n",
    "\n",
    "            self.train = tf.train.AdamOptimizer(learning_rate=0.000833).minimize(self.cost)\n",
    "\n",
    "            self.predict = tf.argmax(self.H,1)\n",
    "            self.correct=tf.equal(self.predict , tf.argmax(self.Y,1)) \n",
    "\n",
    "            self.accuracy=tf.reduce_sum(tf.cast(self.correct , dtype=tf.float32))\n",
    "\n",
    "\n",
    "    def train_model(self,x,y,keep):\n",
    "        \n",
    "        _,cost_val=self.sess.run([self.train,self.cost], feed_dict={self.X:x , self.Y:y , self.keep_rate:keep})\n",
    "        return cost_val\n",
    "    \n",
    "    def ensemble_predict(self,x,keep):\n",
    "        H_val = self.sess.run(self.H, feed_dict={self.X:x,self.keep_rate:keep})\n",
    "        return H_val\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "count_of_models = 10\n",
    "models = [CNNModel(sess,\"model\"+ str(x)) for x in range(count_of_models)]\n",
    "keep_rate=0.5\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#앙상블\n",
    "for num1 in range(count_of_models):     # 10\n",
    "    print(models[num1].build_graph)\n",
    "    print(\"======\")\n",
    "    print(models[num1])\n",
    "start_roof=0\n",
    "increase_roof=100\n",
    "finish_roof=100\n",
    "batch_size=100\n",
    "dataindex_len=len(x_data)  # 42000\n",
    "traindataindex_len=int(dataindex_len/batch_size)  # 420\n",
    "\n",
    "\n",
    "H_zero = np.zeros((100,10),dtype=np.float32)\n",
    "print(\"모델이름: model{}\".format(num1))\n",
    "for num2 in range(traindataindex_len):   # 420\n",
    "    batch_x = x_data[start_roof:finish_roof]\n",
    "    batch_y = y_data[start_roof:finish_roof]\n",
    "\n",
    "    for num3 in range(10):\n",
    "        model_ensemble=models[num1].ensemble_predict(batch_x , keep_rate) # 앞수들 변수내용같은건 텐서플로 내에서 저장\n",
    "\n",
    "        H_zero=H_zero+model_ensemble[num1]\n",
    "\n",
    "        start_roof=start_roof+increase_roof\n",
    "        finish_roof=finish_roof+increase_roof  \n",
    "    ensemble_predict = tf.argmax(H_zero,1)\n",
    "    ensemble_correct=tf.equal(ensemble_predict , tf.argmax(batch_y,1)) \n",
    "\n",
    "    ensemble_accuracy=tf.reduce_sum(tf.cast(ensemble_correct , dtype=tf.float32))\n",
    "    ensemble_accuracy_final+=sess.run(ensemble_accuracy)\n",
    "print(\"모델10개를 {}개씩 , 정확도 : {}\".format(num1,ensemble_accuracy_final)) #  20190321 오류인듯\n",
    "\n",
    "\n",
    "\n",
    "#학습\n",
    "# for num1 in range(count_of_models):    \n",
    "#     print(models[num1].build_graph)\n",
    "#     print(\"======\")\n",
    "#     print(models[num1])\n",
    "#     start_roof=0\n",
    "#     increase_roof=100\n",
    "#     finish_roof=100\n",
    "#     batch_size=100\n",
    "#     dataindex_len=len(x_data)  # 42000\n",
    "#     traindataindex_len=int(dataindex_len/batch_size)  # 420\n",
    "#     model_pt=[\"m1\",\"m2\",\"m3\",\"m4\"\"m5\",\"m6\"\"m7\"\"m8\"\"m9\",\"m10\"]\n",
    "#     print(\"모델이름: model{}\".format(num1))\n",
    "#     for num2 in range(traindataindex_len):\n",
    "#         batch_x = x_data[start_roof:finish_roof]\n",
    "#         batch_y = y_data[start_roof:finish_roof]\n",
    "#         model_pt[num1]=models[num1].train_model(batch_x , batch_y , keep_rate) # 앞수들 변수내용같은건 텐서플로 내에서 저장\n",
    "#         start_roof=start_roof+increase_roof\n",
    "#         finish_roof=finish_roof+increase_roof  \n",
    "    \n",
    "    \n",
    "#     print(\"모델이름 : model{} , cost : {}\".format(num1,model_pt[num1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#학습완료\n",
    "    \n",
    "###// 여기까지 완료    \n",
    "    \n",
    "# for num in range(count_of_models):\n",
    "#     models[num].build_graph()\n",
    "    \n",
    "#     print(models[num])\n",
    "    \n",
    "#     start_roof=0\n",
    "#     increase_roof=100\n",
    "#     finish_roof=100\n",
    "#     batch_size=100\n",
    "#     dataindex_len=len(x_data)\n",
    "#     traindataindex_len=int(dataindex_len/batch_size)\n",
    "    \n",
    "#     print(traindataindex_len)\n",
    "#     H_zero = np.zeros((100,10),dtype=np.float32)\n",
    "#     n=0\n",
    "#     n1=100\n",
    "#     n2=100\n",
    "#     keep_rate=0.5\n",
    "    \n",
    "#     for step in range(10):\n",
    "\n",
    "#         start_roof=0\n",
    "#         increase_roof=100\n",
    "#         finish_roof=100\n",
    "\n",
    "#         for i in range(traindataindex_len):\n",
    "#             batch_x = x_data[start_roof:finish_roof]\n",
    "#             batch_y = y_data[start_roof:finish_roof]\n",
    "#             models[num].train_model(batch_x , batch_y , keep_rate)\n",
    "#             start_roof=start_roof+increase_roof\n",
    "#             finish_roof=finish_roof+increase_roof\n",
    "\n",
    "#         print(\"cost : {}\".format(cost_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "===============\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "모델이름: model0\n",
      "m0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-b4880cfab97f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_ensemble\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[0mmodel_ensemble\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_x\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mkeep_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 앞수들 변수내용같은건 텐서플로 내에서 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m             \u001b[0mH_zero\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mH_zero\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmodel_ensemble\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "## reset tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "### x_data , y_data -> dataframe 형태\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train = pd.read_csv(\"./data/mnist2/train.csv\")\n",
    "test = pd.read_csv(\"./data/mnist2/test.csv\")\n",
    "\n",
    "\n",
    "x_data=train.drop(\"label\", axis=1, inplace=False)\n",
    "train2=train[\"label\"]\n",
    "\n",
    " \n",
    "df1=pd.DataFrame()\n",
    "arr = np.zeros((10,10), dtype=np.int32)\n",
    "df1=arr\n",
    "df2=pd.DataFrame()\n",
    "for n in range(0,10):\n",
    "    df1[n,n]=1\n",
    "    df2[n]=np.arange(10)\n",
    "\n",
    "tr2=int(len(train2))\n",
    "for n in range(0,tr2):\n",
    "    df2[n]=df1[train2[n]]\n",
    "y_data=pd.DataFrame()\n",
    "y_data=df2.transpose()\n",
    "\n",
    "number=int(len(x_data)*2/3)\n",
    "\n",
    "xtrain_data=x_data[0:number].values\n",
    "xtext_data=x_data[number:-1].values\n",
    "\n",
    "ytrain_data=y_data[0:number].values\n",
    "ytest_data=y_data[number:-1].values\n",
    "print(\"===============\")\n",
    "print(xtrain_data)\n",
    "x_data=x_data.values\n",
    "y_data=y_data.values\n",
    "\n",
    "\n",
    "print(\"===============\")\n",
    "\n",
    "### x_data , y_data -> dataframe 형태 -> 넘파이 어레이가 공통적으로 쓰임\n",
    "# display(x_data)\n",
    "# print(\"============\")\n",
    "# display(y_data) #성공\n",
    "\n",
    "#클레스 생성\n",
    "\n",
    "class CNNModel: \n",
    "    \n",
    "    def __init__(self,sess,name):     \n",
    "\n",
    "        self.sess=sess\n",
    "        self.name=name \n",
    "        self.build_graph()            # 해주고 안해주고 차이가 뭐지?\n",
    "    def build_graph(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            self.X=tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "            self.Y=tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "            self.keep_rate=tf.placeholder(dtype=tf.float32) \n",
    "            \n",
    "            X_img = tf.reshape(self.X, shape=[-1,28,28,1])\n",
    "\n",
    "            W1=tf.Variable(tf.random_normal([5,5,1,32],stddev=0.01))\n",
    "\n",
    "            L1 = tf.nn.conv2d(X_img,W1,strides=[1,1,1,1], padding=\"SAME\")\n",
    "\n",
    "            L1 = tf.nn.relu(L1)\n",
    "            p1 = tf.nn.max_pool(L1 , ksize=[1,2,2,1] , strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "            L2=tf.layers.conv2d(inputs=p1, filters=64, kernel_size=[5,5] , padding=\"SAME\", strides=1 ,activation=tf.nn.relu)\n",
    "\n",
    "            L2=tf.layers.max_pooling2d(inputs=L2 , pool_size=[2,2], padding=\"SAME\" , strides=2)\n",
    "\n",
    "            L2=tf.reshape(L2,shape=[-1,7*7*64])\n",
    "\n",
    "            W2=tf.get_variable(\"weight2\" , shape=[7*7*64,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2=tf.Variable(tf.random_normal([256]),name=\"bias2\" )\n",
    "\n",
    "            _layer1 = tf.nn.relu(tf.matmul(L2,W2)+b2 )\n",
    "            layer1 = tf.nn.dropout(_layer1 , keep_prob=self.keep_rate)\n",
    "\n",
    "\n",
    "            W3=tf.get_variable(\"weight3\" , shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b3=tf.Variable(tf.random_normal([256]),name=\"bias3\" )\n",
    "\n",
    "            _layer2 = tf.nn.relu(tf.matmul(layer1,W3)+b3 )\n",
    "            layer2 = tf.nn.dropout(_layer2 , keep_prob=self.keep_rate)\n",
    "\n",
    "\n",
    "            W4=tf.get_variable(\"weight4\" , shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b4=tf.Variable(tf.random_normal([10]),name=\"bias4\" )\n",
    "\n",
    "            self.H = tf.matmul(layer2,W4)+b4\n",
    "\n",
    "            self.cost= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.H , labels=self.Y))\n",
    "\n",
    "            self.train = tf.train.AdamOptimizer(learning_rate=0.000833).minimize(self.cost)\n",
    "\n",
    "            self.predict = tf.argmax(self.H,1)\n",
    "            self.correct=tf.equal(self.predict , tf.argmax(self.Y,1)) \n",
    "\n",
    "            self.accuracy=tf.reduce_sum(tf.cast(self.correct , dtype=tf.float32))\n",
    "\n",
    "\n",
    "    def train_model(self,x,y,keep):\n",
    "        \n",
    "        _,cost_val=self.sess.run([self.train,self.cost], feed_dict={self.X:x , self.Y:y , self.keep_rate:keep})\n",
    "        return cost_val\n",
    "    \n",
    "    def ensemble_predict(self,x,keep):\n",
    "        H_val = self.sess.run(self.H, feed_dict={self.X:x,self.keep_rate:keep})\n",
    "        return H_val\n",
    "sess = tf.Session()\n",
    "count_of_models = 10\n",
    "models = [CNNModel(sess,\"model\"+ str(x)) for x in range(count_of_models)]\n",
    "keep_rate=0.5\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "H_zero = np.zeros((100,10),dtype=np.float32)\n",
    "print(H_zero)\n",
    "\n",
    "#앙상블\n",
    "    \n",
    "for num1 in range(count_of_models):    \n",
    "#     models[num1].build_graph\n",
    "#     print(\"======\")\n",
    "#     print(models[num1])\n",
    "    start_roof=0\n",
    "    increase_roof=100\n",
    "    finish_roof=100\n",
    "    batch_size=100\n",
    "    dataindex_len=len(x_data)  # 42000\n",
    "    traindataindex_len=int(dataindex_len/batch_size)  # 420\n",
    "    model_ensemble=[\"m0\",\"m1\",\"m2\",\"m3\",\"m4\",\"m5\",\"m6\",\"m7\",\"m8\",\"m9\"]\n",
    "    \n",
    "    H_zero = np.zeros((100,10),dtype=np.float32)\n",
    "    print(\"모델이름: model{}\".format(num1))\n",
    "    for num2 in range(traindataindex_len):\n",
    "        batch_x = x_data[start_roof:finish_roof]\n",
    "        batch_y = y_data[start_roof:finish_roof]\n",
    "        \n",
    "        for num3 in range(0,9,1):\n",
    "            models[num3].build_graph\n",
    "            print(model_ensemble[num3])\n",
    "            \n",
    "            model_ensemble[num3]=(models[num3].ensemble_predict(batch_x , keep_rate)) # 앞수들 변수내용같은건 텐서플로 내에서 저장\n",
    "\n",
    "            H_zero=H_zero+model_ensemble[num3]\n",
    "\n",
    "        H_zero=H_zero/10    \n",
    "        ensemble_predict = tf.argmax(H_zero,1)\n",
    "        ensemble_correct=tf.equal(ensemble_predict , tf.argmax(batch_y,1)) \n",
    "\n",
    "        ensemble_accuracy=tf.reduce_sum(tf.cast(ensemble_correct , dtype=tf.float32))\n",
    "        ensemble_accuracy=sess.run(ensemble_accuracy)\n",
    "        start_roof=start_roof+increase_roof\n",
    "        finish_roof=finish_roof+increase_roof  \n",
    "        print(\"모델10개를 {}번씩 , 정확도 : {}\".format(num2,ensemble_accuracy))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-5053c2a7fc32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_ensemble\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnum3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "len(model_ensemble[num3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
