{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image\n",
      "(1, 3, 3, 1)\n",
      "w.shape\n",
      "(2, 2, 1, 3)\n",
      "=====\n",
      "(1, 2, 2, 3)\n",
      "[[[[ 12. 120.  36.]\n",
      "   [ 16. 160.  48.]]\n",
      "\n",
      "  [[ 24. 240.  72.]\n",
      "   [ 28. 280.  84.]]]]\n",
      "[[[[ 12. 120.  36.]\n",
      "   [ 16. 160.  48.]]\n",
      "\n",
      "  [[ 24. 240.  72.]\n",
      "   [ 28. 280.  84.]]]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 원본사진\n",
    "# 1,3,3,1\n",
    "# 이미지 개수 ,가로 ,세로 ,뎁스  \n",
    "\n",
    "#즉 4차원 배열이됨\n",
    "\n",
    "#image shape => (1,3,3,1)\n",
    "\n",
    "image=np.array([\n",
    "                [\n",
    "                    [\n",
    "                        [1],\n",
    "                        [2],\n",
    "                        [3]\n",
    "                    ],\n",
    "                    [\n",
    "                        [4],\n",
    "                        [5],\n",
    "                        [6]\n",
    "                    ],\n",
    "                    [\n",
    "                        [7],\n",
    "                        [8],\n",
    "                        [9]\n",
    "                    ],\n",
    "                 ]\n",
    "                ], dtype=np.float32 )\n",
    "\n",
    "# 필터는\n",
    "# 2,2,1,1\\\n",
    "# 가로 ,세로 ,색상 ,필터개수\n",
    "\n",
    "W=np.array([[ [[1,10,3]],[[1,10,3]] ],[[[1,10,3]],[[1,10,3]] ]] , dtype=np.float32)\n",
    "\n",
    "print(\"image\")\n",
    "print(image.shape)\n",
    "print(\"w.shape\")\n",
    "print(W.shape)\n",
    "print(\"=====\")\n",
    "conv2d=tf.nn.conv2d(image,W ,strides=[1,1,1,1],padding=\"VALID\"  ) #컨벌루션 결과 만들어내는것 첫\n",
    "# 앞에가 움직이는 칸수? 맨뒤가 뎁스인듯?  가운데 두개가 스트라이트 가로 세로 크기고\n",
    "print(conv2d.shape)\n",
    "#컨벌루션 만들기\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(conv2d))\n",
    "#렐루처리\n",
    "sess.run(tf.global_variables_initializer())\n",
    "conv2d2= tf.nn.relu(conv2d)\n",
    "\n",
    "print(sess.run(conv2d2))\n",
    "\n",
    "\n",
    "#conv2d=tf.layers.conv2d( inputs=image ,filters=32 , kernel_size=[2,2],padding=\"VALID\",strides=1 )\n",
    "#랜덤으로만들어진 필터 32개를 쓰겠다 , 커널사이즈 =>필터의 가로 세로\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'ndims'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-138-0afe2b1a28cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mconv2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mfilters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"VALID\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[0;32m    415\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m       _scope=name)\n\u001b[1;32m--> 417\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    815\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \"\"\"\n\u001b[1;32m--> 817\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m         \u001b[1;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    731\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m           \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m           \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_ndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1462\u001b[0m           spec.max_ndim is not None):\n\u001b[1;32m-> 1463\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1464\u001b[0m           raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\u001b[0;32m   1465\u001b[0m                            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'ndims'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "image=np.array([\n",
    "                [\n",
    "                    [\n",
    "                        [1],\n",
    "                        [2],\n",
    "                        [3]\n",
    "                    ],\n",
    "                    [\n",
    "                        [4],\n",
    "                        [5],\n",
    "                        [6]\n",
    "                    ],\n",
    "                    [\n",
    "                        [7],\n",
    "                        [8],\n",
    "                        [9]\n",
    "                    ],\n",
    "                 ]\n",
    "                ], dtype=np.float32 )\n",
    "\n",
    "\n",
    "\n",
    "conv2d=tf.layers.conv2d(image ,filters=32 , kernel_size=[2,2],padding=\"VALID\",strides=1,activation=tf.nn.relu )\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 2, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[4],\n",
       "         [3]],\n",
       "\n",
       "        [[2],\n",
       "         [1]]]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "image=np.array([[[[4],[3]],[[2],[1]]]])\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "\n",
    "pool = tf.nn.max_pool(image,ksize=[1,2,2,1],strides=[1,1,1,1],padding=\"SAME\"  ) # 이미지.ksize=>가운데 두개가 커널사이즈\n",
    "\n",
    "sess.run(pool)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "<tf.Variable 'Variable_13:0' shape=(3, 3, 1, 5) dtype=float32_ref>\n",
      "[[[[ 0.564848    0.21784659 -0.05179293  1.0026104   0.6142582 ]]\n",
      "\n",
      "  [[ 0.47037178 -2.000208    0.05126894 -0.01366385  0.0381602 ]]\n",
      "\n",
      "  [[ 1.2183268  -0.02804831 -1.274778   -0.8446352   0.31649688]]]\n",
      "\n",
      "\n",
      " [[[-1.8669828  -1.5103611   1.1043017   0.516193   -0.82981503]]\n",
      "\n",
      "  [[-0.3571028   0.74557096 -0.7620549   0.7729514  -0.4136966 ]]\n",
      "\n",
      "  [[-1.6174588   0.44974217  1.5934393   0.8787012  -0.36248267]]]\n",
      "\n",
      "\n",
      " [[[ 0.7859718   0.18238261  0.4968576  -0.7630445   0.6595795 ]]\n",
      "\n",
      "  [[ 0.62737596  0.23209378  1.4978089   0.2831002   1.0188973 ]]\n",
      "\n",
      "  [[-0.25165033  0.58463234  0.20035347  1.4663912  -0.02353636]]]]\n",
      "(1, 14, 14, 5)\n",
      "(5, 14, 14, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "##1 data loading\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "                                  \n",
    "##2 처음 한장가지고 해보기\n",
    "\n",
    "img=mnist.train.images[0]\n",
    "\n",
    "#plt.imshow(img.reshape(28,28), cmap=\"Greys\")\n",
    "#plt.show()\n",
    "\n",
    "#원본데이터의 형태부터 변경\n",
    "img=tf.reshape(img,shape=[-1,28,28,1])\n",
    "\n",
    "\n",
    "#필터를 정의 필터크기 (3,3)    ,   필터개수 5 ,     컬러와 매핑\n",
    "\n",
    "W=tf.Variable(tf.random_normal([3,3,1,5]))\n",
    "\n",
    "print(W)\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(W))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conv2d = tf.nn.conv2d(img,W,strides=[1,2,2,1], padding=\"SAME\") #스트라이드가 2칸씩뛰어서 same이어도 원본크기와 다르다\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "result=sess.run(conv2d)\n",
    "print(result.shape) #즉1,14,14,5   14,14크기의  이미지가 5장\n",
    "\n",
    "\n",
    "#배열의 형태를 원하는 축으로 바꿀수있음\n",
    "\n",
    "#결과이미지를 출력해보기위해 데이터처리를 진행\n",
    "#axes 변경 (5,14,14,1)\n",
    "result = np.swapaxes(result,0,3)\n",
    "print(result.shape)\n",
    "\n",
    "\n",
    "fig, axes=plt.subplots(1,5) # 1행 5열짜리 그림 5개를 보여줄수있는 영역을 잡음 idx -> 0~4까지 돔 3차원의 데이터가 t_img\n",
    "\n",
    "for idx,t_img in enumerate(result):\n",
    "    axes[idx].imshow(t_img.reshape(14,14) , cmap=\"Greys\")\n",
    "    \n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "<tf.Variable 'Variable_16:0' shape=(3, 3, 1, 5) dtype=float32_ref>\n",
      "[[[[ 0.70456785  1.5333397  -0.31133732 -0.9093865  -1.2263485 ]]\n",
      "\n",
      "  [[ 0.7592716  -0.7934383  -0.33679467  0.11619866 -0.48332092]]\n",
      "\n",
      "  [[-2.354685   -1.1978384  -0.9500589   0.9130684   1.5428164 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.19052754 -0.55516255  0.08739493 -0.13432261  1.2451923 ]]\n",
      "\n",
      "  [[ 0.5317349  -1.8491356  -1.0913033  -0.66508824 -0.7311274 ]]\n",
      "\n",
      "  [[-1.1620284  -0.6636378  -0.34935746 -1.1681707  -0.93551135]]]\n",
      "\n",
      "\n",
      " [[[-1.2350953  -1.536322   -0.19770922  0.66953534 -0.05007719]]\n",
      "\n",
      "  [[ 0.00294905  0.20925233 -0.05548407  0.15297288  0.17352991]]\n",
      "\n",
      "  [[ 0.07570092 -1.2994522  -1.7848533   0.56283915 -1.0497148 ]]]]\n",
      "(1, 7, 7, 5)\n",
      "(5, 7, 7, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#사이즈 줄이기 풀링 순서는 빈도는 개발자 마음대로이고 정해진게 없다 대신 컨벌루션 렐루 는 같이해야함\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "##1 data loading\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
    "                                  \n",
    "##2 처음 한장가지고 해보기\n",
    "\n",
    "img=mnist.train.images[0]\n",
    "\n",
    "#plt.imshow(img.reshape(28,28), cmap=\"Greys\")\n",
    "#plt.show()\n",
    "\n",
    "#원본데이터의 형태부터 변경\n",
    "img=tf.reshape(img,shape=[-1,28,28,1])\n",
    "\n",
    "\n",
    "#필터를 정의 필터크기 (3,3)    ,   필터개수 5 ,     컬러와 매핑\n",
    "\n",
    "W=tf.Variable(tf.random_normal([3,3,1,5]))\n",
    "\n",
    "print(W)\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print(sess.run(W))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "conv2d = tf.nn.conv2d(img,W,strides=[1,2,2,1], padding=\"SAME\") #스트라이드가 2칸씩뛰어서 same이어도 원본크기와 다르다\n",
    "conv2d = tf.nn.relu(conv2d)\n",
    "\n",
    "pool = tf.nn.max_pool(conv2d , ksize=[1,2,2,1] , strides=[1,2,2,1], padding=\"SAME\")\n",
    "\n",
    "\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "result=sess.run(pool)\n",
    "print(result.shape) #즉1,14,14,5   14,14크기의  이미지가 5장\n",
    "\n",
    "\n",
    "#배열의 형태를 원하는 축으로 바꿀수있음\n",
    "\n",
    "#결과이미지를 출력해보기위해 데이터처리를 진행\n",
    "#axes 변경 (5,14,14,1)\n",
    "result = np.swapaxes(result,0,3)\n",
    "print(result.shape)\n",
    "\n",
    "\n",
    "fig, axes=plt.subplots(1,5) # 1행 5열짜리 그림 5개를 보여줄수있는 영역을 잡음 idx -> 0~4까지 돔 3차원의 데이터가 t_img\n",
    "\n",
    "for idx,t_img in enumerate(result):\n",
    "    axes[idx].imshow(t_img.reshape(7,7) , cmap=\"Greys\")\n",
    "    \n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "WARNING:tensorflow:From <ipython-input-2-8e600e35ce2a>:13: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./data/mnist\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\gpu_env\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "(55000, 784)\n",
      "Tensor(\"Reshape:0\", shape=(?, 28, 28, 1), dtype=float32)\n",
      "Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "L2\n",
      "(?, 14, 14, 64)\n",
      "(?, 7, 7, 64)\n",
      "L2[0]\n",
      "Tensor(\"max_pooling2d/MaxPool:0\", shape=(?, 7, 7, 64), dtype=float32)\n",
      "55000\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
      "cost : 0.11918853968381882\n",
      "cost : 0.025524070486426353\n",
      "cost : 0.0319925919175148\n",
      "cost : 0.1206931322813034\n",
      "cost : 0.0809931680560112\n",
      "cost : 0.018962763249874115\n",
      "cost : 0.019394071772694588\n",
      "cost : 0.013427503407001495\n",
      "cost : 0.022194460034370422\n",
      "cost : 0.004096765071153641\n",
      "정확도:0.9933\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "#mnist with cnn\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data   #패키지는 from\n",
    "\n",
    "tf.reset_default_graph()  #충돌방지 텐서플로 그래프 초기화\n",
    "\n",
    "\n",
    "#1 데이터불러오기\n",
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True) \n",
    "#데이터가 없으면 (즉 전처리를 해준상태의 데이터) 자동으로 만들어줌 -> 편리하게 쓸수있도록 해줌\n",
    "\n",
    "img=mnist.train.images\n",
    "print(img.shape)\n",
    "#2 데이터를 받아야함 , 입력을 받아야하거나할 때-> 플레이스 홀더\n",
    "X=tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
    "Y=tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
    "\n",
    "keep_rate=tf.placeholder(dtype=tf.float32)  #드랍아웃에서 얼마나 살릴지\n",
    "\n",
    "#3 convolution layer\n",
    "#3.1 convolution layer\n",
    "# 입력 데이터의 형태를 convolution 할수 있도록 4차배열로 reshape 그럼 입력데이터의 형태를 보는방법이 뭐였지?\n",
    "# 들어오는 데이터 X의 shape = None,784\n",
    "X_img = tf.reshape(X, shape=[-1,28,28,1])\n",
    "\n",
    "print(X_img)\n",
    "\n",
    "W1=tf.Variable(tf.random_normal([5,5,1,32],stddev=0.01))\n",
    "#3.2 convolution layer\n",
    "\n",
    "L1 = tf.nn.conv2d(X_img,W1,strides=[1,1,1,1], padding=\"SAME\")\n",
    "#RELU\n",
    "\n",
    "L1 = tf.nn.relu(L1)\n",
    "\n",
    "print(L1)\n",
    "p1 = tf.nn.max_pool(L1 , ksize=[1,2,2,1] , strides=[1,2,2,1], padding=\"SAME\")\n",
    "print(p1)\n",
    "\n",
    "\n",
    "\n",
    "#layer two 다른형태 위에꺼 복사해도됨\n",
    "# W2=tf.Variable(tf.random_normal([5,5,1,32],stddev=0.01))\n",
    "# L1 = tf.nn.conv2d(p1,W1,strides=[1,1,1,1], padding=\"SAME\") 이런식으로\n",
    "\n",
    "\n",
    "#filter,convolution,relu\n",
    "\n",
    "L2=tf.layers.conv2d(inputs=p1, filters=64, kernel_size=[5,5] , padding=\"SAME\", strides=1 ,activation=tf.nn.relu)\n",
    "\n",
    "\n",
    "\n",
    "print(\"L2\")\n",
    "print(L2.shape)\n",
    "\n",
    "L2=tf.layers.max_pooling2d(inputs=L2 , pool_size=[2,2], padding=\"SAME\" , strides=2)\n",
    "\n",
    "print(L2.shape)\n",
    "\n",
    "#########CONVOLUTION LAYER 끝\n",
    "\n",
    "#4. NURAL NETWORK\n",
    "\n",
    "print(\"L2[0]\")\n",
    "\n",
    "print(L2)\n",
    "\n",
    "L2=tf.reshape(L2,shape=[-1,7*7*64])\n",
    "\n",
    "#print(L2.shape)\n",
    "\n",
    "# 5. weight & bias\n",
    "\n",
    "W2=tf.get_variable(\"weight2\" , shape=[7*7*64,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2=tf.Variable(tf.random_normal([256]),name=\"bias2\" )\n",
    "\n",
    "_layer1 = tf.nn.relu(tf.matmul(L2,W2)+b2 )\n",
    "layer1 = tf.nn.dropout(_layer1 , keep_prob=keep_rate)\n",
    "\n",
    "\n",
    "W3=tf.get_variable(\"weight3\" , shape=[256,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3=tf.Variable(tf.random_normal([256]),name=\"bias3\" )\n",
    "\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W3)+b3 )\n",
    "layer2 = tf.nn.dropout(_layer2 , keep_prob=keep_rate)\n",
    "\n",
    "\n",
    "W4=tf.get_variable(\"weight4\" , shape=[256,10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4=tf.Variable(tf.random_normal([10]),name=\"bias4\" )\n",
    "\n",
    "# hypothesis 가설만들어야함\n",
    "\n",
    "\n",
    "H = tf.matmul(layer2,W4)+b4\n",
    "\n",
    "#cost 함수\n",
    "\n",
    "cost= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=H , labels=Y))\n",
    "\n",
    "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#학습\n",
    "\n",
    "num_of_epoch = 10\n",
    "batch_size=100\n",
    "\n",
    "print(mnist.train.num_examples)\n",
    "batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "\n",
    "print(batch_y)\n",
    "\n",
    "\n",
    "for step in range(num_of_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)\n",
    "    for i in range(num_of_iter):\n",
    "        batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
    "        _,cost_val=sess.run([train,cost], feed_dict={X:batch_x , Y:batch_y ,keep_rate:0.5})\n",
    "\n",
    "        \n",
    "    print(\"cost : {}\".format(cost_val))\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "predict = tf.argmax(H,1)\n",
    "correct=tf.equal(predict , tf.argmax(Y,1)) \n",
    "\n",
    "accuracy=tf.reduce_sum(tf.cast(correct , dtype=tf.float32))\n",
    "\n",
    "result_sum=0\n",
    "num_of_iter = int(mnist.test.num_examples/batch_size)\n",
    "for i in range(num_of_iter):\n",
    "    batch_x,batch_y = mnist.test.next_batch(batch_size)\n",
    "    correct_num=sess.run(accuracy, feed_dict={X:batch_x , Y:batch_y ,keep_rate:1})\n",
    "    result_sum += correct_num\n",
    "    \n",
    "#tf.cast(correct , dtype=tf.float32# 맞으면 1 틀리면 0 100개를 끌어왔는데 반은맞고 반은 틀리면 50이됨 \n",
    "#print(\"정확도:{}\".format(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels,keep_rate:1})) )\n",
    "\n",
    "print(\"정확도:{}\".format(result_sum/10000))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
