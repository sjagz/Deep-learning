{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n",
      "Extracting ./Data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "0.71490115\n",
      "0.52129847\n",
      "0.3381587\n",
      "0.28896254\n",
      "0.30678657\n",
      "0.21497002\n",
      "0.21150793\n",
      "0.1287621\n",
      "0.24726202\n",
      "0.32844788\n",
      "==========\n",
      "0.9577\n"
     ]
    }
   ],
   "source": [
    "## MNIST -Multinomial Classification#relu#Xavier initialization 도입 초기 W값 지정#Drop out(overfitting 방지)\n",
    "%time\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "#W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "keep = tf.placeholder(dtype = tf.float32) #입력 값으로 사용하겠다. 상수로 박지 않고. \n",
    "W1 = tf.get_variable(\"wight1\",shape = [784,256], initializer=  tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep) #keepprob 유지할 확률 0.5~0.7\n",
    "#W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "W2 = tf.get_variable(\"wight2\",shape = [256,512], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep)\n",
    "#W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "W3 = tf.get_variable(\"wight3\",shape = [512,10], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "H = tf.matmul(layer2,W3) +b3\n",
    "# H = tf.nn.softmax(logit)안해도 됨\n",
    "\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = H,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y,keep:0.7})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "\n",
    "#정확도 출력\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels, keep:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
