{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0], [0], [0], [1]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'keep_rate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-36023f75cab2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_anddata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_anddata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkeep_rate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m \u001b[1;33m}\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m300\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cost:{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keep_rate' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "\n",
    "#and 연산 \n",
    "\n",
    "x_anddata= [[0,0],\n",
    "            [0,1],\n",
    "            [1,0],\n",
    "            [1,1]]\n",
    "y_anddata=[[0],[0],[0],[1]]\n",
    "\n",
    "print(y_anddata)\n",
    "\n",
    "X = tf.placeholder(shape=[None,2],dtype = tf.float32  ) #행 숫자는 무상관\n",
    "Y = tf.placeholder(shape=[None,1],dtype = tf.float32  )\n",
    "\n",
    "W= tf.Variable(tf.random_normal([2,1]), name=\"weight\"  )\n",
    "b= tf.Variable(tf.random_normal([1]),name=\"bias\")\n",
    "\n",
    "logit = tf.matmul(X,W)+b\n",
    "\n",
    "H=tf.sigmoid(logit)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits( logits=logit , labels=Y ))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for step in range(3000):\n",
    "    _,cost_val = sess.run([train,cost], feed_dict={X:x_anddata,Y:y_anddata,keep_rate:0.5 } )\n",
    "    if step%300 == 0 :\n",
    "        print(\"cost:{}\".format(cost_val))\n",
    "\n",
    "predict = tf.cast(H>0.5 ,dtype=tf.float32) #원래는 정수를 실수로 , 실수를 정수로 바꿔주는 cast 타입변환 1.0 0.0 으로 변환인듯?\n",
    "                                            #0.5보다 크면 1로 떨어짐\n",
    "correct = tf.equal(predict,Y) #실데이터와 예측한 것을 비교하는것 \n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct , dtype=tf.float32))\n",
    "\n",
    "print(\"정확도 ={}\".format(sess.run(accuracy ,feed_dict={X:x_anddata,Y:y_anddata} ) ))\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost:2.7065720558166504\n",
      "cost:0.00013740842405240983\n",
      "cost:8.53808960528113e-05\n",
      "cost:6.267914432100952e-05\n",
      "cost:4.983652615919709e-05\n",
      "cost:4.154649650445208e-05\n",
      "cost:3.5734479752136394e-05\n",
      "cost:3.1429590308107436e-05\n",
      "cost:2.8106787794968113e-05\n",
      "cost:2.5461282348260283e-05\n",
      "정확도 =1.0\n",
      "예측값=[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# ## Multiple layer 를 이용한 XOR문제 해결\n",
    "\n",
    "# import tensorflow as tf\n",
    "\n",
    "# # 1. Training data set\n",
    "\n",
    "# x_data = [[0,0],\n",
    "#          [0,1],\n",
    "#          [1,0],\n",
    "#          [1,1]]\n",
    "\n",
    "# y_data = [[0],[1],[1],[0]]\n",
    "\n",
    "# #2. placeHolder\n",
    "\n",
    "# X = tf.placeholder(shape = [None,2], dtype = tf.float32)\n",
    "# Y = tf.placeholder(shape = [None,1], dtype = tf.float32)\n",
    "\n",
    "# #3. Weight &bias\n",
    "# W1 = tf.Variable(tf.random_normal([2,256]), name = \"whight1\") #한 레이어에서 wide 하게 20개로 늘림. 2->20 로지스틱 숫자 늘리기\n",
    "# b1 = tf.Variable(tf.random_normal([256]), name = \"bias1\") #출력 256개\n",
    "\n",
    "# layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "# W2 = tf.Variable(tf.random_normal([256,512]), name = \"whight2\") #한 레이어에서 wide 하게 20개로 늘림. 512 = 로지스틱 숫자 , 클수록 정확\n",
    "# b2 = tf.Variable(tf.random_normal([512]), name = \"bias2\") #출력 256개\n",
    "\n",
    "# layer2 = tf.sigmoid(tf.matmul(layer1,W2) + b2) #depts 3개 딥 네트워크. 너무 깊게 하면 오히려 학습이 안되는 경우가 있다. \n",
    "\n",
    "# #학습이 너무 잘되면 over fitting 이 발생, 실제 예측이 엇나감. 적정 선을 유지해야 합니다. \n",
    "\n",
    "# W3 = tf.Variable(tf.random_normal([512,1]), name = \"whight3\") #행렬 곱 연산이 일어나도록 수정#입력 256개\n",
    "# b3 = tf.Variable(tf.random_normal([1]), name = \"bias3\") #최종 값은 하나\n",
    "\n",
    "# #4. Hypothesis\n",
    "# logit = tf.matmul(layer2,W3) + b3 \n",
    "# H = tf.sigmoid(logit)\n",
    "\n",
    "# cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,labels=Y ))\n",
    "\n",
    "# train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "# sess=tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# for step in range(30000):\n",
    "#     _,cost_val = sess.run([train,cost], feed_dict={X:x_data , Y:y_data})\n",
    "    \n",
    "#     if step%3000==0 :\n",
    "#         print(\"cost:{}\".format(cost_val))\n",
    "\n",
    "# predict = tf.cast(H>0.5 ,dtype=tf.float32) #원래는 정수를 실수로 , 실수를 정수로 바꿔주는 cast 타입변환 1.0 0.0 으로 변환인듯?\n",
    "#                                             #0.5보다 크면 1로 떨어짐\n",
    "# correct = tf.equal(predict,Y) #실데이터와 예측한 것을 비교하는것 \n",
    "\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct , dtype=tf.float32))\n",
    "\n",
    "\n",
    "# print(\"정확도 ={}\".format(sess.run(accuracy ,feed_dict={X:x_data,Y:y_data} ) ))\n",
    "\n",
    "\n",
    "# #prediction 예측\n",
    "\n",
    "# print(\"예측값={}\".format(sess.run(predict,feed_dict={X:x_data,Y:y_data})))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "3.6376514\n",
      "2.0265338\n",
      "1.2301319\n",
      "1.0275302\n",
      "1.1596653\n",
      "0.6793493\n",
      "0.73589736\n",
      "0.6498156\n",
      "0.49743897\n",
      "0.6619822\n",
      "0.8542\n"
     ]
    }
   ],
   "source": [
    "## MNIST -Multinomial Classification\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "logit = tf.matmul(layer2,W3) +b3\n",
    "H = tf.nn.softmax(logit)\n",
    "#지금은 layer추가하고 있다는 점 유의.\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "#정확도 출력\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "21.089388\n",
      "17.186964\n",
      "2.3570757\n",
      "0.0\n",
      "0.42308196\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "==========\n",
      "0.9999818\n",
      "==========\n",
      "0.9409\n"
     ]
    }
   ],
   "source": [
    "## MNIST -Multinomial Classification#relu\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "H = tf.matmul(layer2,W3) +b3\n",
    "# H = tf.nn.softmax(logit)안해도 됨\n",
    "\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = H,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.train.images,Y:mnist.train.labels}))\n",
    "#정확도 출력\n",
    "print(\"=\"*10)\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "0.6762993\n",
      "0.41321594\n",
      "0.3357047\n",
      "0.34630498\n",
      "0.25484332\n",
      "0.18033093\n",
      "0.10422319\n",
      "0.22170722\n",
      "0.093595535\n",
      "0.09794306\n",
      "==========\n",
      "0.9594\n"
     ]
    }
   ],
   "source": [
    "## MNIST -Multinomial Classification#relu#Xavier initialization 도입 초기 W값 지정\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "#W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "W1 = tf.get_variable(\"wight1\",shape = [784,256], initializer=  tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "\n",
    "#W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "W2 = tf.get_variable(\"wight2\",shape = [256,512], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "#W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "W3 = tf.get_variable(\"wight3\",shape = [512,10], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "H = tf.matmul(layer2,W3) +b3\n",
    "# H = tf.nn.softmax(logit)안해도 됨\n",
    "\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = H,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "\n",
    "#정확도 출력\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./Data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./Data/mnist\\t10k-labels-idx1-ubyte.gz\n",
      "0.8976872\n",
      "0.61608267\n",
      "0.25697765\n",
      "0.42058846\n",
      "0.4338282\n",
      "0.18917851\n",
      "0.23240094\n",
      "0.14603838\n",
      "0.15598248\n",
      "0.3351928\n",
      "==========\n",
      "0.9578\n"
     ]
    }
   ],
   "source": [
    "## MNIST -Multinomial Classification#relu#Xavier initialization 도입 초기 W값 지정#Drop out(overfitting 방지)\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "#1. Data Loading\n",
    "mnist = input_data.read_data_sets(\"./Data/mnist\",one_hot = True) #one hot형태의 y 측 데이터 로딩\n",
    "\n",
    "#2. Placeholer\n",
    "##입력 데이터는 image data, 3차원 가로 세로 칼럼(depts 3 rgb color,빨노초 3레이어 지금은 흑백이어서 2차원 data.)\n",
    "##2차원 데이터 이미지 데이터. \n",
    "#원래는 이미지 개수 가로 세로 칼라.-> 가로세로를 pixel data로 제공 = 이미지 데이터를 1차원으로 제공 28*28 = 784개의 열\n",
    "#, 칼라도 흑백으로 생략 -> 2차원\n",
    "\n",
    "X = tf.placeholder(shape = [None,784], dtype = tf.float32)\n",
    "Y = tf.placeholder(shape = [None,10], dtype = tf.float32) #one hot 인코딩 y label\n",
    "\n",
    "#3. Weight &bias\n",
    "# W = tf.Variable(tf.random_normal([784,10]), name =\"weight\")\n",
    "# b = tf.Variable(tf.random_normal([10]), name =\"bias\")\n",
    "#W1 = tf.Variable(tf.random_normal([784,256]), name =\"weight1\")\n",
    "keep = tf.placeholder(dtype = tf.float32) #입력 값으로 사용하겠다. 상수로 박지 않고. \n",
    "W1 = tf.get_variable(\"wight1\",shape = [784,256], initializer=  tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]), name =\"bias1\")\n",
    "_layer1 = tf.nn.relu(tf.matmul(X,W1)+b1)\n",
    "layer1 = tf.nn.dropout(_layer1, keep_prob = keep) #keepprob 유지할 확률 0.5~0.7\n",
    "#W2 = tf.Variable(tf.random_normal([256,512]), name =\"weight2\")\n",
    "W2 = tf.get_variable(\"wight2\",shape = [256,512], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]), name =\"bias2\")\n",
    "_layer2 = tf.nn.relu(tf.matmul(layer1,W2)+b2)\n",
    "layer2 = tf.nn.dropout(_layer2, keep_prob = keep)\n",
    "#W3 = tf.Variable(tf.random_normal([512,10]), name =\"weight3\")\n",
    "W3 = tf.get_variable(\"wight3\",shape = [512,10], initializer= tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]), name =\"bias3\")\n",
    "\n",
    "#4. Hypothesis\n",
    "# logit = tf.matmul(X,W) + b\n",
    "# H = tf.nn.sotfmax(logit) #확률값으로 결과를 얻는 방법\n",
    "H = tf.matmul(layer2,W3) +b3\n",
    "# H = tf.nn.softmax(logit)안해도 됨\n",
    "\n",
    "\n",
    "# 5. Cost function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = H,labels=Y))\n",
    "\n",
    "#6. Train\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "#7. Session & 초기화\n",
    "sess=tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#8.학습   => 이전의 for문 방법은 데이터가 작기때문에 가능했다, but 데이터가 커지면 대치처리를 해줘야 함\n",
    "train_epoch = 30   #우리가 가지고 있는 데이터를 가지고 n번 학습하는 것 = n epoch\n",
    "batch_size = 100   #한번에 읽어들일 데이터의 크기, 몇개씩 잘라서 들고올건가\n",
    "for step in range(train_epoch):\n",
    "    num_of_iter = int(mnist.train.num_examples/batch_size)  #반복횟수 : 전체데이터 / batch_size \n",
    "    \n",
    "    for i in range(num_of_iter):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)       #전체의 훈련데이터(mnist.train)에서 데이터를 100개씩 가져옴\n",
    "        _, cost_val= sess.run([train, cost],feed_dict={X:batch_x,Y:batch_y,keep:0.7})\n",
    "        \n",
    "    if step % 3 == 0:\n",
    "        print(cost_val)\n",
    "        \n",
    "#accuracy\n",
    "predict=tf.argmax(H,1)\n",
    "correct=tf.equal(predict, tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
    "print(\"=\"*10)\n",
    "\n",
    "#정확도 출력\n",
    "print(sess.run(accuracy, feed_dict={X:mnist.test.images,Y:mnist.test.labels, keep:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_env",
   "language": "python",
   "name": "data_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
